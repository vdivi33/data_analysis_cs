{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency\n",
    "from dython.nominal import associations\n",
    "# Read in data from CSV\n",
    "df = pd.read_csv('file.csv')\n",
    "\n",
    "# Basic DataFrame operations\n",
    "df.head()           # View first 5 rows\n",
    "df.tail()           # View last 5 rows\n",
    "df.describe()       # Summary statistics for numerical columns\n",
    "df.info()           # Info on column data types and missing values\n",
    "df.shape            # Get number of rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Data_Analytics_Take_Home_HL_repaired.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "df.drop(['Column1', 'Column2'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'OldName': 'NewName'}, inplace=True)\n",
    "\n",
    "# Check for and sum missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Fill missing values\n",
    "df['Column'].fillna(df['Column'].mean(), inplace=True)  # Fill with mean\n",
    "df['Column'].fillna(df['Column'].median(), inplace=True)  # Fill with median\n",
    "df['Column'].fillna('Value', inplace=True)  # Fill with specific value\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert data types\n",
    "df['Column'] = df['Column'].astype('int')\n",
    "df['Column'] = df['Column'].astype('float')\n",
    "df['Column'] = df['Column'].astype('category')\n",
    "\n",
    "# String operations\n",
    "df['Column'] = df['Column'].str.lower()  # Convert to lower case\n",
    "df['Column'] = df['Column'].str.upper()  # Convert to upper case\n",
    "df['Column'] = df['Column'].str.strip()  # Remove whitespace\n",
    "\n",
    "# Datetime operations\n",
    "df['DateColumn'] = pd.to_datetime(df['DateColumn'])\n",
    "df['Year'] = df['DateColumn'].dt.year\n",
    "df['Month'] = df['DateColumn'].dt.month\n",
    "df['Day'] = df['DateColumn'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply functions\n",
    "df['Column'] = df['Column'].apply(lambda x: x+1)\n",
    "\n",
    "# Operations on columns\n",
    "df['NewColumn'] = df['Column1'] + df['Column2']\n",
    "df['NewColumn'] = df['Column1'] * df['Column2']\n",
    "\n",
    "# Aggregations\n",
    "df.groupby('Column').sum()\n",
    "df.groupby('Column').mean()\n",
    "df.groupby('Column').agg({'Column2': 'sum', 'Column3': 'mean'})\n",
    "\n",
    "# Sorting\n",
    "df.sort_values(by='Column', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot\n",
    "plt.plot(df['Column1'], df['Column2'])\n",
    "plt.title('Title')\n",
    "plt.xlabel('X-axis Label')\n",
    "plt.ylabel('Y-axis Label')\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "df['Column'].hist(bins=50)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(df['Column1'], df['Column2'])\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "df.boxplot(column=['Column1', 'Column2'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "grouped_counts = data.groupby(['page_category', 'page_topic_description']).size().unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy variables for categorical columns\n",
    "df_encoded = pd.get_dummies(df, columns=['CategoricalColumn'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conversion_rate(data, column_name):\n",
    "    conversions = data[data['Conversion'] == 1].groupby(['Asset Shown', column_name])['Conversion'].count()\n",
    "    \n",
    "    total_shown = data.groupby(['Asset Shown', column_name])['Conversion'].count()\n",
    "    \n",
    "    conversion_rate = (conversions / total_shown)\n",
    "    \n",
    "    sorted_conversion_rate = conversion_rate.sort_values(ascending=False)\n",
    "    print(sorted_conversion_rate)\n",
    "    print('\\n')\n",
    "\n",
    "    \n",
    "def calculate_revenue(data, column_name):\n",
    "    converted_data = data[data['Conversion'] == 1]\n",
    "    asset_revenues = np.array([5, 7, 2.5])\n",
    "    conversions_table = converted_data.groupby([column_name, 'Asset Shown']).size().reset_index(name='counts')\n",
    "    conversions = conversions_table['counts'].to_numpy()\n",
    "\n",
    "    repeated_revenues = np.tile(asset_revenues, int(np.ceil(len(conversions)/len(asset_revenues))))[:len(conversions)]\n",
    "    conversions_table['Revenue'] = conversions * repeated_revenues\n",
    "\n",
    "    revenue_by_column = conversions_table.groupby(column_name)['Revenue'].sum().reset_index().sort_values(by='Revenue', ascending=False)\n",
    "    return revenue_by_column, conversions_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshuffle_data(data):\n",
    "    majority_class = data[data['Conversion'] == 0]\n",
    "    minority_class = data[data['Conversion'] == 1]\n",
    "\n",
    "    # Undersample the rows of no conversion\n",
    "    majority_class_undersampled = majority_class.sample(len(minority_class), random_state=13)\n",
    "\n",
    "    # Concatenate the randomly sampled rows\n",
    "    balanced_data = pd.concat([majority_class_undersampled, minority_class], axis=0)\n",
    "\n",
    "    # shuffle the rows\n",
    "    balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_constant = sm.add_constant(data['time_difference'])\n",
    "model = sm.Logit(data['Conversion'], predictor_constant).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Preprocess and clean the data\n",
    "# ... (handle missing values, correct data types, etc.)\n",
    "\n",
    "# One-hot encoding\n",
    "data = pd.get_dummies(data, columns=['Known Diagnosis', 'Page Category', 'Device Type', 'Page Topic Description', 'Asset Shown'])\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "# ... (calculate average conversion rates, visualize distributions, etc.)\n",
    "\n",
    "# Prepare the data for modeling\n",
    "X = data.drop(['Conversion', 'Revenue', 'Other non-predictive columns'], axis=1)\n",
    "y = data['Conversion']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, probabilities)\n",
    "\n",
    "# Calculate expected revenue for each combination\n",
    "data['Expected Revenue'] = probabilities * data['Revenue per Conversion']\n",
    "\n",
    "# Find the best asset for each user/page combination\n",
    "data['Best Asset'] = data[['Expected Revenue A', 'Expected Revenue B', 'Expected Revenue C']].idxmax(axis=1)\n",
    "\n",
    "# Report on findings\n",
    "# ... (summarize variable importance, lucrative combinations, etc.)\n",
    "\n",
    "# Calculate incremental revenue\n",
    "# ... (estimate potential revenue gains from optimal asset matching)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
